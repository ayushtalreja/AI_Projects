{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"**Referenznotebook zum Thema Natural Language Processing. Für die vollständige Erklärungen bitte auch das Video nutzen.**\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"In dieser Vorlesung werden wir anhand eines Beispiels veranschaulichen wie man R für Natural Language Processing verwenden kann. Es sind keine Aufgaben und auch kein Projekt beigefügt da dieses Thema zu umfangreich ist. Du wirst vermutlich auch bemerken, dass R nicht unbedingt die beste Wahl für NLP ist. Python eignet sich hier wegen dem Library Support meist besser.\\n\",\n",
    "    \"Falls du dennoch mehr Informationen möchtest, kannst du diese [hier](http://www.mjdenny.com/Text_Processing_In_R.html) finden\\n\"\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"## Installation der benötigten Libraries \\n\",\n",
    "    \"\\n\",\n",
    "    \"Wir benötigen die folgenden Libraries:\\n\",\n",
    "    \"* tm\\n\",\n",
    "    \"* twitteR\\n\",\n",
    "    \"* wordcloud\\n\",\n",
    "    \"* RColorBrewer\\n\",\n",
    "    \"* e1017\\n\",\n",
    "    \"* class\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"Du kannst sie mit diesem Code installieren (erst auskommentieren):\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"#install.packages('tm',repos='http://cran.us.r-project.org')\\n\",\n",
    "    \"#install.packages('twitteR',repos='http://cran.us.r-project.org')\\n\",\n",
    "    \"#install.packages('wordcloud',repos='http://cran.us.r-project.org')\\n\",\n",
    "    \"#install.packages('RColorBrewer',repos='http://cran.us.r-project.org')\\n\",\n",
    "    \"#install.packages('e1017',repos='http://cran.us.r-project.org')\\n\",\n",
    "    \"#install.packages('class',repos='http://cran.us.r-project.org')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"# Eine Twitter App erstellen\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"Für dieses Projekt benötigst du einen Twitter Account und eine Twitter Applikation. Hier die benötigten Schritte: \\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Einen Account auf Twitter erstellen\\n\",\n",
    "    \"2. Eine neue App erstellen auf: https://apps.twitter.com/\\n\",\n",
    "    \"3. Eventuell musst du es auf eine persönliche URL verweisen. Hierfür kannst du z.B. eine Wordpress Seite oder etwas Ähnliches erstellen. \\n\",\n",
    "    \"4. Erhalte deine Keys bei dem **Keys and Access Tokens** tab\\n\",\n",
    "    \"5. Verwende diese Keys mit der TwitteR Library: \\n\",\n",
    "    \"\\n\",\n",
    "    \"        getTwitterOAuth(consumer_key, consumer_secret)\\n\",\n",
    "    \"\\n\",\n",
    "    \"____\\n\",\n",
    "    \"## Übersicht der Regular Expression \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \"Jetzt werden wir einige wichtige Regualr Expression Funktionen anschauen die wir schon behandelt haben:\\n\",\n",
    "    \"\\n\",\n",
    "    \"____\\n\",\n",
    "    \"### grep()\\n\",\n",
    "    \"\\n\",\n",
    "    \"Gebe den Index der Musterübereinstimmung zurück \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"args(grep)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"grep('A', c('A','B','C','D','A'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"### nchar()\\n\",\n",
    "    \"Länge des Strings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"args(nchar)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"nchar('helloworld')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"nchar('hello world')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"### gsub()\\n\",\n",
    "    \"\\n\",\n",
    "    \"Führe jetzt den Austausch des passenden Musters durch\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"args(gsub)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"gsub('pattern','replacement','hello have you seen the pattern here?')\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"## Text Manipulation\\n\",\n",
    "    \"\\n\",\n",
    "    \"### paste()\\n\",\n",
    "    \"\\n\",\n",
    "    \"mehrere Strings verbinden\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"print(paste('A','B','C',sep='...'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"#help(paste)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"### substr()\\n\",\n",
    "    \"\\n\",\n",
    "    \"Gebe den Substring zurück der im gegebenen Abschnitt start:stop liegt\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"substr('abcdefg',start=2,stop = 5)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"Teile den String in eine Liste von Substrings basierend auf einem anderen String x, der angibt wo der ursprüngliche String geteilt werden soll. \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"strsplit('2016-01-23',split='-')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"source\": [\n",
    "    \"_____\\n\",\n",
    "    \"# NLP Wichtige Begriffe und Konzepte xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\n\",\n",
    "    \"\\n\",\n",
    "    \"Important Terms and Concepts\\n\",\n",
    "    \"\\n\",\n",
    "    \"* Document - Das individuelle Textdokument \\n\",\n",
    "    \"* Corpus - Die Sammlung an Textdokumenten\\n\",\n",
    "    \"* Bag-of-Words - Ungeordnete Sammlung an Wörtern\\n\",\n",
    "    \"* n-grams - Zusammenhängende Sequenz von n Gegenständen von einer gegebenen Textsequenz \\n\",\n",
    "    \"\\n\",\n",
    "    \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx contiguous sequence of n items from a given sequence of text (e.g. ['A','G','C',T']\\n\",\n",
    "    \"\\n\",\n",
    "    \"* Stopwords - Stoppwörter sind sehr häufig auftauchenden Wörter einer Sprache, die gleichzeitig nicht wichtig für den wesentlichen Gehalt eines Texts sind. \\n\",\n",
    "    \"* Tokens - Kombination aus Zeichen (Wörter)\\n\",\n",
    "    \"* Stemming - Prozess bei den wir Silben von Wörtern entfernen (z.b. run,runner,running auf das Basiswort \\\"run\\\" reduzieren)\\n\",\n",
    "    \"* TF-IDF : \\\"Term Frequency-Inver Document Frequency\\\" ist ein statistisches Maß, dass dazu dient auszuwerten, wie wichtig ein Wort in einem Dokument in einem Corpus ist.\\n\",\n",
    "    \"* Term Document Matrix - Representation einer Dokumentensammlung als Vektoren \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"# Twitter Mining\\n\",\n",
    "    \"\\n\",\n",
    "    \"Lass uns Twitter generellen Daten untersuchen:\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Schritt 1:  Libraries importieren\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"library(twitteR)\\n\",\n",
    "    \"library(tm)\\n\",\n",
    "    \"library(wordcloud)\\n\",\n",
    "    \"library(RColorBrewer)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \"## Schritt 2: Nach einem Thema auf Twitter suchen\\n\",\n",
    "    \"\\n\",\n",
    "    \"Wir werden die twitteR Library verwenden um twitter zu untersuchen. Zu Beginn musst du dich verbinden, indem du die Autorizations Keys und die Tokens einstellst.\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"setup_twitter_oauth(consumer_key, consumer_secret, access_token=NULL, access_secret=NULL)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"Wir werden Twitter nach dem Wort 'soccer' untersuchen: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \"soccer.tweets <- searchTwitter(\\\"soccer\\\", n=2000, lang=\\\"en\\\")\\n\",\n",
    "    \"soccer.text <- sapply(soccer.tweets, function(x) x$getText())\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \"## Schritt 3: Daten Bereinigen\\n\",\n",
    "    \"\\n\",\n",
    "    \"Wir entfernen die Emoticons und erstellen einen Corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"soccer.text <- iconv(soccer.text, 'UTF-8', 'ASCII') # Emoticons enfernen\\n\",\n",
    "    \"soccer.corpus <- Corpus(VectorSource(soccer.text)) # Corpus erstellen\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \"## Schritt 4: Erstelle die Document Term Matrix \\n\",\n",
    "    \"\\n\",\n",
    "    \"Wir werden ein paar Transformationen anwenden mithilfe der `TermDocumentMatrix` Funktion\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"term.doc.matrix <- TermDocumentMatrix(soccer.corpus,\\n\",\n",
    "    \"                                      control = list(removePunctuation = TRUE,\\n\",\n",
    "    \"                                                     stopwords = c(\\\"soccer\\\",\\\"http\\\", stopwords(\\\"english\\\")),\\n\",\n",
    "    \"                                                     removeNumbers = TRUE,tolower = TRUE))\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \"## Schritt 5: Überprüfung der Matrix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"head(term.doc.matrix)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"term.doc.matrix <- as.matrix(term.doc.matrix)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"## Schritt 6: Anzahl der Wörter bekommen\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \"word.freqs <- sort(rowSums(term.doc.matrix), decreasing=TRUE) \\n\",\n",
    "    \"dm <- data.frame(word=names(word.freqs), freq=word.freqs)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \"## Schritt 7: Eine Word Cloud erstellen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, \\\"Dark2\\\"))\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"# Gut gemacht!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
