{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftZOo86bzVLC"
      },
      "source": [
        "# Road Follower - Train Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yB8Xj6QrzVLF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import glob\n",
        "import PIL.Image\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-ajuXIxzVLG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoEBCFhNzVLH"
      },
      "outputs": [],
      "source": [
        "!unzip -q road_following.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmeOt1DkzVLH"
      },
      "source": [
        "You should see a folder named ``dataset_all`` appear in the file browser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Netz4e4XzVLH"
      },
      "source": [
        "### Create Dataset Instance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRV-fiqqzVLI"
      },
      "outputs": [],
      "source": [
        "def get_x(path, width):\n",
        "    \"\"\"Gets the x value from the image filename\"\"\"\n",
        "    return (float(int(path.split(\"_\")[0])) - width/2) / (width/2)\n",
        "\n",
        "def get_y(path, height):\n",
        "    \"\"\"Gets the y value from the image filename\"\"\"\n",
        "    return (float(int(path.split(\"_\")[1])) - height/2) / (height/2)\n",
        "\n",
        "class XYDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, directory, random_hflips=True):\n",
        "        self.directory = directory\n",
        "        self.random_hflips = random_hflips\n",
        "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
        "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "\n",
        "        image = PIL.Image.open(image_path)\n",
        "        width, height = image.size\n",
        "        x = float(get_x(os.path.basename(image_path), width))\n",
        "        y = float(get_y(os.path.basename(image_path), height))\n",
        "\n",
        "        if float(np.random.rand(1)) > 0.5:\n",
        "            image = transforms.functional.hflip(image)\n",
        "            x = -x\n",
        "\n",
        "        image = self.color_jitter(image)\n",
        "        image = transforms.functional.resize(image, (224, 224))\n",
        "        image = transforms.functional.to_tensor(image)\n",
        "        image = image.numpy()[::-1].copy()\n",
        "        image = torch.from_numpy(image)\n",
        "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "        return image, torch.tensor([x, y]).float()\n",
        "\n",
        "dataset = XYDataset('/content/sample_data/road_following', random_hflips=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.__getitem__(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbbioedH5rrq",
        "outputId": "59534d57-18bb-4bc5-83cd-3726d19639ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.7077,  0.8447,  0.8276,  ...,  0.3309,  0.3481,  0.3823],\n",
              "          [ 0.7077,  0.8276,  0.8104,  ...,  0.3481,  0.3481,  0.3309],\n",
              "          [ 0.7248,  0.8276,  0.8104,  ...,  0.3481,  0.3309,  0.2967],\n",
              "          ...,\n",
              "          [-0.5424, -0.5596, -0.5082,  ..., -0.8678, -0.8678, -0.7993],\n",
              "          [-0.4911, -0.5596, -0.5424,  ..., -0.8507, -0.8164, -0.7822],\n",
              "          [-0.4226, -0.4739, -0.5082,  ..., -0.8849, -0.8507, -0.8335]],\n",
              " \n",
              "         [[ 0.4678,  0.6078,  0.5903,  ...,  0.3627,  0.3627,  0.4153],\n",
              "          [ 0.4678,  0.5728,  0.5728,  ...,  0.3627,  0.3627,  0.3627],\n",
              "          [ 0.4853,  0.5728,  0.5728,  ...,  0.3627,  0.3627,  0.3277],\n",
              "          ...,\n",
              "          [-0.5826, -0.6176, -0.5476,  ..., -0.8102, -0.7927, -0.7227],\n",
              "          [-0.5301, -0.6352, -0.5826,  ..., -0.7927, -0.7402, -0.7052],\n",
              "          [-0.4951, -0.5126, -0.5476,  ..., -0.8277, -0.7752, -0.7577]],\n",
              " \n",
              "         [[ 1.8383,  1.9777,  2.0300,  ...,  1.3851,  1.4025,  1.4374],\n",
              "          [ 1.8383,  1.9603,  2.0125,  ...,  1.4025,  1.4025,  1.3851],\n",
              "          [ 1.8731,  1.9603,  2.0125,  ...,  1.4025,  1.3851,  1.3502],\n",
              "          ...,\n",
              "          [ 0.3219,  0.3045,  0.3045,  ..., -0.0441, -0.0267,  0.0431],\n",
              "          [ 0.3742,  0.2871,  0.2871,  ..., -0.0267,  0.0256,  0.0605],\n",
              "          [ 0.4265,  0.3916,  0.3045,  ..., -0.0615, -0.0092,  0.0082]]]),\n",
              " tensor([-0.4554, -0.0446]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxuCoxWyzVLI"
      },
      "source": [
        "### Split dataset into train and test sets\n",
        "Once we read dataset, we will split data set in train and test sets. In this example we split train and test a 90%-10%. The test set will be used to verify the accuracy of the model we train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BBQjvP8zVLJ"
      },
      "outputs": [],
      "source": [
        "test_percent = 0.1\n",
        "num_test = int(test_percent * len(dataset))\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.__getitem__(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je_Iom8QImRI",
        "outputId": "9180230b-8973-4f3c-aa30-400186742470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.0569,  0.0398,  0.0741,  ...,  1.1700,  0.6563, -0.0458],\n",
              "          [ 0.0569,  0.0398,  0.0569,  ...,  1.1529,  0.8961,  0.0056],\n",
              "          [ 0.0398,  0.0398,  0.0398,  ...,  1.2385,  1.1358,  0.0398],\n",
              "          ...,\n",
              "          [-0.7822, -0.7137, -0.6452,  ..., -0.5938, -0.5938, -0.6281],\n",
              "          [-0.7137, -0.7137, -0.6109,  ..., -0.5938, -0.6281, -0.5938],\n",
              "          [-0.7137, -0.6281, -0.6281,  ..., -0.6281, -0.6794, -0.5938]],\n",
              " \n",
              "         [[ 0.6954,  0.7129,  0.7304,  ...,  1.5182,  1.4657,  0.6604],\n",
              "          [ 0.6954,  0.7129,  0.7129,  ...,  1.5182,  1.4657,  0.6954],\n",
              "          [ 0.7129,  0.7129,  0.7129,  ...,  1.5532,  1.5182,  0.6779],\n",
              "          ...,\n",
              "          [-0.3725, -0.2675, -0.2325,  ..., -0.1275, -0.1275, -0.1800],\n",
              "          [-0.3025, -0.2850, -0.1975,  ..., -0.1275, -0.1625, -0.1275],\n",
              "          [-0.3025, -0.2500, -0.2325,  ..., -0.1625, -0.2150, -0.1275]],\n",
              " \n",
              "         [[ 0.5834,  0.6008,  0.6182,  ...,  1.8034,  1.6814,  0.9668],\n",
              "          [ 0.5834,  0.6008,  0.6008,  ...,  1.8034,  1.7511,  0.9842],\n",
              "          [ 0.6008,  0.6008,  0.6008,  ...,  1.8208,  1.8034,  0.9319],\n",
              "          ...,\n",
              "          [-0.3230, -0.2184, -0.1835,  ...,  0.0082, -0.0092, -0.0441],\n",
              "          [-0.2358, -0.2184, -0.1487,  ...,  0.0082, -0.0267, -0.0092],\n",
              "          [-0.2358, -0.1661, -0.1661,  ..., -0.0267, -0.0790, -0.0092]]]),\n",
              " tensor([ 0.2321, -0.1071]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AAD3PTvzVLJ"
      },
      "source": [
        "### Create data loaders to load data in batches\n",
        "\n",
        "We use ``DataLoader`` class to load data in batches, shuffle data and allow using multi-subprocesses. In this example we use batch size of 64. Batch size will be based on memory available with your GPU and it can impact accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C399uCWQzVLK"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2R-K3OlzVLK"
      },
      "source": [
        "### Define Neural Network Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Sf6MBKzVLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5890046c-d68a-4df9-875f-0fadb0b58fc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
            "100%|██████████| 83.3M/83.3M [00:04<00:00, 18.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = models.resnet34(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgKvP9DGzVLK"
      },
      "source": [
        "ResNet model has fully connect (fc) final layer with 512 as ``in_features`` and we will be training for regression thus ``out_features`` as 1\n",
        "\n",
        "Finally, we transfer our model for execution on the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KLzhYmuzVLL"
      },
      "outputs": [],
      "source": [
        "model.fc = torch.nn.Linear(512, 2)\n",
        "device = torch.device('cuda')\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erluFLT2zVLL"
      },
      "source": [
        "### Train Regression:\n",
        "\n",
        "We train for 128 epochs and save best model if the loss is reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqK8cRsLzVLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c652971-26e6-4a37-bd76-d4d8ad82e500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.362684, 3.634238\n",
            "0.046558, 0.108073\n",
            "0.046285, 0.049332\n",
            "0.046538, 0.077803\n",
            "0.049761, 0.048316\n",
            "0.042374, 0.083923\n",
            "0.036310, 0.058541\n",
            "0.042996, 0.070533\n",
            "0.044609, 0.663793\n",
            "0.049235, 0.058433\n",
            "0.041492, 0.066871\n",
            "0.039952, 0.050553\n",
            "0.033130, 0.059794\n",
            "0.030449, 0.050611\n",
            "0.040957, 0.056327\n",
            "0.028264, 0.058429\n",
            "0.035377, 0.117736\n",
            "0.031974, 0.066163\n",
            "0.032475, 0.076066\n",
            "0.034096, 0.064949\n",
            "0.033460, 0.076201\n",
            "0.035110, 0.085631\n",
            "0.042439, 0.057268\n",
            "0.037849, 0.084455\n",
            "0.031720, 0.060031\n",
            "0.030640, 0.055386\n",
            "0.030914, 0.044305\n",
            "0.029916, 0.074270\n",
            "0.030316, 0.052997\n",
            "0.026362, 0.068530\n",
            "0.028430, 0.062915\n",
            "0.028864, 0.064330\n",
            "0.027196, 0.070942\n",
            "0.026747, 0.068407\n",
            "0.026055, 0.064152\n",
            "0.025494, 0.055487\n",
            "0.027124, 0.052963\n",
            "0.024731, 0.054004\n",
            "0.027424, 0.065252\n",
            "0.025094, 0.058245\n",
            "0.024122, 0.066426\n",
            "0.025415, 0.080543\n",
            "0.025099, 0.059544\n",
            "0.027787, 0.058061\n",
            "0.025721, 0.073814\n",
            "0.023806, 0.067620\n",
            "0.028325, 0.074456\n",
            "0.026848, 0.066154\n",
            "0.025620, 0.062824\n",
            "0.025451, 0.075221\n",
            "0.029414, 0.091437\n",
            "0.026046, 0.078287\n",
            "0.023778, 0.131052\n",
            "0.026948, 0.080611\n",
            "0.029084, 0.098637\n",
            "0.024103, 0.074151\n",
            "0.036870, 0.093015\n",
            "0.032835, 0.079802\n",
            "0.026761, 0.072331\n",
            "0.025956, 0.071378\n",
            "0.025725, 0.073618\n",
            "0.029535, 0.138174\n",
            "0.025806, 0.068575\n",
            "0.025362, 0.055115\n",
            "0.027004, 0.081775\n",
            "0.023299, 0.094665\n",
            "0.027603, 0.063574\n",
            "0.025515, 0.246282\n",
            "0.027408, 0.072157\n",
            "0.026374, 0.063835\n",
            "0.021995, 0.081426\n",
            "0.021028, 0.072087\n",
            "0.025740, 0.081422\n",
            "0.026864, 0.121688\n",
            "0.028017, 0.071682\n",
            "0.024915, 0.077350\n",
            "0.027357, 0.087258\n",
            "0.022214, 0.082300\n",
            "0.021547, 0.098295\n",
            "0.024885, 0.059749\n",
            "0.023039, 0.070424\n",
            "0.025837, 0.083931\n",
            "0.032377, 0.088490\n",
            "0.028656, 0.104843\n",
            "0.024852, 0.072828\n",
            "0.021562, 0.082639\n",
            "0.020868, 0.071132\n",
            "0.022003, 0.077056\n",
            "0.021025, 0.079821\n",
            "0.019257, 0.072813\n",
            "0.019824, 0.078014\n",
            "0.023398, 0.076569\n",
            "0.021062, 0.084052\n",
            "0.023305, 0.082618\n",
            "0.021706, 0.068653\n",
            "0.017557, 0.079466\n",
            "0.021081, 0.076564\n",
            "0.015783, 0.105972\n",
            "0.022274, 0.091751\n",
            "0.017731, 0.067786\n",
            "0.016837, 0.080183\n",
            "0.014293, 0.095197\n",
            "0.015919, 0.066937\n",
            "0.012586, 0.076725\n",
            "0.011933, 0.064463\n",
            "0.013406, 0.078346\n",
            "0.015200, 0.089152\n",
            "0.014854, 0.073246\n",
            "0.012444, 0.081002\n",
            "0.011362, 0.079499\n",
            "0.010570, 0.063040\n",
            "0.008765, 0.077069\n",
            "0.008313, 0.075705\n",
            "0.008961, 0.087557\n",
            "0.007814, 0.062567\n",
            "0.006825, 0.075078\n",
            "0.006750, 0.079325\n",
            "0.005457, 0.071175\n",
            "0.005350, 0.077033\n",
            "0.004500, 0.091018\n",
            "0.005503, 0.071548\n",
            "0.004022, 0.078779\n",
            "0.003541, 0.081860\n",
            "0.005476, 0.092646\n",
            "0.004997, 0.072888\n",
            "0.005062, 0.075008\n",
            "0.003784, 0.075478\n",
            "0.003052, 0.079267\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 128\n",
        "BEST_MODEL_PATH = 'best_steering_model_xy_resnet34.pth'\n",
        "best_loss = 1e9\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for images, labels in iter(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = F.mse_loss(outputs, labels)\n",
        "        train_loss += float(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    for images, labels in iter(test_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = F.mse_loss(outputs, labels)\n",
        "        test_loss += float(loss)\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    print('%f, %f' % (train_loss, test_loss))\n",
        "    if test_loss < best_loss:\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        best_loss = test_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch2trt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPDO2j8nTNrR",
        "outputId": "3e488eb0-004a-4be2-cbe2-c45a61272642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch2trt (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch2trt\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e68v3q3DzVLM"
      },
      "source": [
        "Once the model is trained, it will generate ``best_steering_model_xy.pth`` file which you can use for inferencing in the live demo notebook.\n",
        "\n",
        "If you trained on a different machine other than JetBot, you'll need to upload this to the JetBot to the ``road_following`` example folder."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}